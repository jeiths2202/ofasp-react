// ê³ ê¸‰ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬í˜„
import { DocumentChunk } from './ragService';

// ë²¡í„° ì„ë² ë”© ì¸í„°í˜ì´ìŠ¤
export interface VectorEmbedding {
  id: string;
  vector: number[];
  metadata: {
    documentId: string;
    chunkIndex: number;
    text: string;
    language: 'ko' | 'ja';
    source: string;
    section?: string;
    timestamp: Date;
  };
}

// ê²€ìƒ‰ ê²°ê³¼ ì¸í„°í˜ì´ìŠ¤
export interface VectorSearchResult {
  embedding: VectorEmbedding;
  similarity: number;
  distance: number;
}

// ë²¡í„° ì„ë² ë”© ìƒì„±ê¸° (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” OpenAI API ì‚¬ìš©)
class EmbeddingGenerator {
  private readonly dimension = 384; // Sentence-BERT í˜¸í™˜ ì°¨ì›
  
  // í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (ê³„ì‚° ì§‘ì•½ì  ì„ë² ë”©)
  async generateEmbedding(text: string, language: 'ko' | 'ja'): Promise<number[]> {
    console.log(`ğŸ§® ë²¡í„° ì„ë² ë”© ìƒì„± ì¤‘... (${text.substring(0, 50)}...)`);
    const startTime = Date.now();
    
    // í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    const normalizedText = this.normalizeText(text, language);
    const tokens = this.tokenize(normalizedText, language);
    
    // ê³„ì‚° ì§‘ì•½ì  ì‘ì—…ë“¤
    const termWeights = this.calculateTermWeights(tokens);
    const contextMatrix = this.buildContextMatrix(tokens, language);
    const semanticFeatures = this.extractSemanticFeatures(tokens, language);
    
    // ë‹¤ì¸µ ë²¡í„° ìƒì„± (ë” ë³µì¡í•œ ê³„ì‚°)
    const baseVector = this.createSemanticVector(tokens, termWeights, language);
    const contextVector = this.applyContextualTransforms(baseVector, contextMatrix);
    const finalVector = this.enhanceWithSemanticFeatures(contextVector, semanticFeatures);
    
    // ì¶”ê°€ ì •ì œ ê³¼ì • (CPU ì‚¬ìš©ëŸ‰ ì¦ê°€ë¥¼ ìœ„í•œ ê³„ì‚°)
    const refinedVector = await this.performVectorRefinement(finalVector, tokens, language);
    
    const processingTime = Date.now() - startTime;
    console.log(`âœ… ë²¡í„° ì„ë² ë”© ì™„ë£Œ (${processingTime}ms, ì°¨ì›: ${refinedVector.length})`);
    
    return this.normalizeVector(refinedVector);
  }
  
  private normalizeText(text: string, language: 'ko' | 'ja'): string {
    // ì–¸ì–´ë³„ ì •ê·œí™”
    let normalized = text.toLowerCase();
    
    if (language === 'ko') {
      // í•œêµ­ì–´ ì •ê·œí™”: ì¡°ì‚¬ ì œê±°, ì–´ê°„ ì¶”ì¶œ ì‹œë®¬ë ˆì´ì…˜
      normalized = normalized
        .replace(/[ì€ëŠ”ì´ê°€ì„ë¥¼ì—ì„œë¡œë¶€í„°ê¹Œì§€ì™€ê³¼]/g, ' ')
        .replace(/ìŠµë‹ˆë‹¤|ã…‚ë‹ˆë‹¤|ì…ë‹ˆë‹¤/g, '')
        .replace(/í–ˆë‹¤|í•œë‹¤|í•˜ë‹¤/g, 'í•˜');
    } else if (language === 'ja') {
      // ì¼ë³¸ì–´ ì •ê·œí™”: ì¡°ì‚¬ ì œê±°, ì–´ë¯¸ ì •ê·œí™”
      normalized = normalized
        .replace(/[ã¯ãŒã‚’ã«ã§ã‹ã‚‰ã¾ã§ã¨ã‚„]/g, ' ')
        .replace(/ã§ã™|ã¾ã™|ã |ã§ã‚ã‚‹/g, '')
        .replace(/ã—ãŸ|ã™ã‚‹|ã•ã‚Œã‚‹/g, 'ã™ã‚‹');
    }
    
    return normalized.replace(/\s+/g, ' ').trim();
  }
  
  private tokenize(text: string, language: 'ko' | 'ja'): string[] {
    // ì–¸ì–´ë³„ í† í°í™”
    if (language === 'ko') {
      // í•œêµ­ì–´: í˜•íƒœì†Œ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜
      return text.split(/\s+/)
        .filter(token => token.length > 1)
        .flatMap(token => this.extractKoreanMorphemes(token));
    } else {
      // ì¼ë³¸ì–´: í˜•íƒœì†Œ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜
      return text.split(/\s+/)
        .filter(token => token.length > 1)
        .flatMap(token => this.extractJapaneseMorphemes(token));
    }
  }
  
  private extractKoreanMorphemes(word: string): string[] {
    // ê°„ë‹¨í•œ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜
    const morphemes = [];
    
    // ë³µí•©ì–´ ë¶„í•´ ì‹œë®¬ë ˆì´ì…˜
    if (word.includes('ê´€ë¦¬')) {
      morphemes.push('ê´€ë¦¬');
    }
    if (word.includes('ì‹œìŠ¤í…œ')) {
      morphemes.push('ì‹œìŠ¤í…œ');
    }
    if (word.includes('ë°ì´í„°ë² ì´ìŠ¤')) {
      morphemes.push('ë°ì´í„°ë² ì´ìŠ¤', 'DB');
    }
    
    morphemes.push(word);
    return morphemes;
  }
  
  private extractJapaneseMorphemes(word: string): string[] {
    // ê°„ë‹¨í•œ ì¼ë³¸ì–´ í˜•íƒœì†Œ ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜
    const morphemes = [];
    
    // ë³µí•©ì–´ ë¶„í•´ ì‹œë®¬ë ˆì´ì…˜
    if (word.includes('ã‚·ã‚¹ãƒ†ãƒ ')) {
      morphemes.push('ã‚·ã‚¹ãƒ†ãƒ ');
    }
    if (word.includes('ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹')) {
      morphemes.push('ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹', 'DB');
    }
    if (word.includes('ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰')) {
      morphemes.push('ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰', 'ãƒ‘ã‚¹');
    }
    
    morphemes.push(word);
    return morphemes;
  }
  
  private calculateTermWeights(tokens: string[]): Map<string, number> {
    const termCount = new Map<string, number>();
    const weights = new Map<string, number>();
    
    // ìš©ì–´ ë¹ˆë„ ê³„ì‚°
    tokens.forEach(token => {
      const count = termCount.get(token) || 0;
      termCount.set(token, count + 1);
    });
    
    // TF ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¡œê·¸ ìŠ¤ì¼€ì¼ë§)
    const maxCount = Math.max(...Array.from(termCount.values()));
    termCount.forEach((count, term) => {
      const tf = 0.5 + 0.5 * (count / maxCount);
      weights.set(term, tf);
    });
    
    return weights;
  }
  
  private createSemanticVector(tokens: string[], weights: Map<string, number>, language: 'ko' | 'ja'): number[] {
    const vector = new Array(this.dimension).fill(0);
    
    tokens.forEach(token => {
      const weight = weights.get(token) || 0;
      const hash = this.stringToHash(token + language);
      
      // í•´ì‹œë¥¼ ì´ìš©í•œ ë¶„ì‚° í‘œí˜„
      for (let i = 0; i < this.dimension; i++) {
        const feature = Math.sin((hash + i) * 0.1) * weight;
        vector[i] += feature;
        
        // ì˜ë¯¸ì  íŠ¹ì„± ì¶”ê°€
        if (this.isImportantTerm(token)) {
          vector[i] += Math.cos((hash + i) * 0.05) * weight * 0.5;
        }
      }
    });
    
    return vector;
  }
  
  private isImportantTerm(token: string): boolean {
    // ì¤‘ìš”í•œ ìš©ì–´ íŒë³„ (ë„ë©”ì¸ íŠ¹í™”)
    const importantTerms = [
      'ASP', 'ã‚·ã‚¹ãƒ†ãƒ ', 'ì‹œìŠ¤í…œ', 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹', 'ë°ì´í„°ë² ì´ìŠ¤',
      'ãƒ­ã‚°ã‚¤ãƒ³', 'ë¡œê·¸ì¸', 'ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰', 'ë¹„ë°€ë²ˆí˜¸', 'ã‚¨ãƒ©ãƒ¼', 'ì˜¤ë¥˜',
      'è¨­å®š', 'ì„¤ì •', 'ç®¡ç†', 'ê´€ë¦¬', 'API', 'ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£', 'ë³´ì•ˆ'
    ];
    
    return importantTerms.some(term => 
      token.includes(term.toLowerCase()) || term.toLowerCase().includes(token)
    );
  }
  
  private stringToHash(str: string): number {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // 32bit ì •ìˆ˜ë¡œ ë³€í™˜
    }
    return Math.abs(hash);
  }
  
  // ì»¨í…ìŠ¤íŠ¸ ë§¤íŠ¸ë¦­ìŠ¤ êµ¬ì„± (ê³„ì‚° ì§‘ì•½ì )
  private buildContextMatrix(tokens: string[], language: 'ko' | 'ja'): number[][] {
    const matrixSize = Math.min(tokens.length, 50); // ìµœëŒ€ 50x50 ë§¤íŠ¸ë¦­ìŠ¤
    const matrix: number[][] = [];
    
    for (let i = 0; i < matrixSize; i++) {
      matrix[i] = [];
      for (let j = 0; j < matrixSize; j++) {
        // í† í° ê°„ ì˜ë¯¸ì  ê±°ë¦¬ ê³„ì‚° (ì‹œë®¬ë ˆì´ì…˜)
        const token1 = tokens[i] || '';
        const token2 = tokens[j] || '';
        const similarity = this.calculateTokenSimilarity(token1, token2, language);
        
        // ê±°ë¦¬ ë³€í™˜ ë° ê°€ì¤‘ì¹˜ ì ìš©
        matrix[i][j] = Math.exp(-similarity * 0.5) * Math.random() * 0.3;
      }
    }
    
    return matrix;
  }
  
  // ì˜ë¯¸ì  íŠ¹ì„± ì¶”ì¶œ (ê³„ì‚° ì§‘ì•½ì )
  private extractSemanticFeatures(tokens: string[], language: 'ko' | 'ja'): number[] {
    const features: number[] = new Array(this.dimension).fill(0);
    
    // ê° í† í°ì— ëŒ€í•´ ë³µì¡í•œ íŠ¹ì„± ê³„ì‚°
    tokens.forEach((token, index) => {
      const tokenHash = this.stringToHash(token);
      
      // ì—¬ëŸ¬ íŠ¹ì„± ë ˆì´ì–´ ê³„ì‚°
      for (let layer = 0; layer < 5; layer++) {
        for (let dim = 0; dim < this.dimension; dim++) {
          const featureValue = Math.sin((tokenHash + dim + layer) * 0.01) * 
                              Math.cos((index + dim + layer) * 0.02) *
                              Math.exp(-layer * 0.1);
          features[dim] += featureValue;
        }
      }
    });
    
    return features;
  }
  
  // ì»¨í…ìŠ¤íŠ¸ ë³€í™˜ ì ìš© (ê³„ì‚° ì§‘ì•½ì )
  private applyContextualTransforms(baseVector: number[], contextMatrix: number[][]): number[] {
    const transformed = [...baseVector];
    const matrixSize = contextMatrix.length;
    
    // ë§¤íŠ¸ë¦­ìŠ¤ ê³±ì…ˆ ì‹œë®¬ë ˆì´ì…˜
    for (let i = 0; i < Math.min(this.dimension, matrixSize); i++) {
      let sum = 0;
      for (let j = 0; j < matrixSize; j++) {
        sum += baseVector[j % this.dimension] * contextMatrix[i][j];
      }
      transformed[i] = (transformed[i] + sum * 0.3) * 0.7;
    }
    
    return transformed;
  }
  
  // ì˜ë¯¸ì  íŠ¹ì„±ìœ¼ë¡œ ë²¡í„° ê°•í™” (ê³„ì‚° ì§‘ì•½ì )
  private enhanceWithSemanticFeatures(vector: number[], semanticFeatures: number[]): number[] {
    const enhanced = [...vector];
    
    for (let i = 0; i < this.dimension; i++) {
      // ë¹„ì„ í˜• ë³€í™˜ ì ìš©
      const originalValue = vector[i];
      const semanticValue = semanticFeatures[i];
      
      enhanced[i] = originalValue * 0.7 + semanticValue * 0.3 + 
                    Math.tanh(originalValue + semanticValue) * 0.1;
    }
    
    return enhanced;
  }
  
  // ë²¡í„° ì •ì œ ê³¼ì • (ë§¤ìš° ê³„ì‚° ì§‘ì•½ì )
  private async performVectorRefinement(vector: number[], tokens: string[], language: 'ko' | 'ja'): Promise<number[]> {
    const refined = [...vector];
    
    // ì—¬ëŸ¬ ë¼ìš´ë“œì˜ ì •ì œ ìˆ˜í–‰
    for (let round = 0; round < 3; round++) {
      console.log(`ğŸ”„ ë²¡í„° ì •ì œ ë¼ìš´ë“œ ${round + 1}/3`);
      
      // ê° ì°¨ì›ì— ëŒ€í•´ ë³µì¡í•œ ê³„ì‚° ìˆ˜í–‰
      for (let dim = 0; dim < this.dimension; dim++) {
        let refinementValue = 0;
        
        // í† í°ë³„ ê¸°ì—¬ë„ ê³„ì‚°
        tokens.forEach((token, tokenIndex) => {
          const tokenWeight = this.calculateTokenWeight(token, language);
          const positionWeight = Math.exp(-tokenIndex * 0.1);
          const dimensionWeight = Math.sin((dim + tokenIndex) * 0.05);
          
          refinementValue += tokenWeight * positionWeight * dimensionWeight;
        });
        
        // ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜ ì ìš©
        refined[dim] = Math.tanh(refined[dim] + refinementValue * 0.1);
      }
      
      // ë¼ìš´ë“œ ê°„ ì§§ì€ ì§€ì—° (CPU ì‚¬ìš©ëŸ‰ ì‹œë®¬ë ˆì´ì…˜)
      await new Promise(resolve => setTimeout(resolve, 10));
    }
    
    return refined;
  }
  
  // í† í° ìœ ì‚¬ë„ ê³„ì‚°
  private calculateTokenSimilarity(token1: string, token2: string, language: 'ko' | 'ja'): number {
    if (token1 === token2) return 1.0;
    
    // ë¬¸ì ê¸°ë°˜ ìœ ì‚¬ë„
    const charSimilarity = this.calculateCharacterSimilarity(token1, token2);
    
    // ì˜ë¯¸ì  ìœ ì‚¬ë„ (ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜)
    const semanticSimilarity = this.calculateSemanticSimilarity(token1, token2, language);
    
    return (charSimilarity * 0.4 + semanticSimilarity * 0.6);
  }
  
  // ë¬¸ì ìœ ì‚¬ë„ ê³„ì‚°
  private calculateCharacterSimilarity(str1: string, str2: string): number {
    const len1 = str1.length;
    const len2 = str2.length;
    const maxLen = Math.max(len1, len2);
    
    if (maxLen === 0) return 1.0;
    
    let matches = 0;
    for (let i = 0; i < Math.min(len1, len2); i++) {
      if (str1[i] === str2[i]) matches++;
    }
    
    return matches / maxLen;
  }
  
  // ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
  private calculateSemanticSimilarity(token1: string, token2: string, language: 'ko' | 'ja'): number {
    // ë„ë©”ì¸ íŠ¹í™” ìœ ì‚¬ì–´ ê·¸ë£¹
    const similarityGroups = language === 'ko' ? [
      ['ì‹œìŠ¤í…œ', 'ì²´ê³„', 'êµ¬ì¡°'],
      ['ê´€ë¦¬', 'ìš´ì˜', 'ì œì–´'],
      ['ì˜¤ë¥˜', 'ì—ëŸ¬', 'ë¬¸ì œ'],
      ['ì„¤ì •', 'êµ¬ì„±', 'í™˜ê²½'],
      ['ì‚¬ìš©ì', 'ìœ ì €', 'ê³„ì •']
    ] : [
      ['ã‚·ã‚¹ãƒ†ãƒ ', 'ä½“ç³»', 'æ§‹é€ '],
      ['ç®¡ç†', 'é‹å–¶', 'åˆ¶å¾¡'],
      ['ã‚¨ãƒ©ãƒ¼', 'èª¤ã‚Š', 'å•é¡Œ'],
      ['è¨­å®š', 'æ§‹æˆ', 'ç’°å¢ƒ'],
      ['ãƒ¦ãƒ¼ã‚¶ãƒ¼', 'åˆ©ç”¨è€…', 'ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ']
    ];
    
    for (const group of similarityGroups) {
      if (group.includes(token1) && group.includes(token2)) {
        return 0.8;
      }
    }
    
    return 0.1;
  }
  
  // í† í° ê°€ì¤‘ì¹˜ ê³„ì‚°
  private calculateTokenWeight(token: string, language: 'ko' | 'ja'): number {
    if (this.isImportantTerm(token)) {
      return 1.5;
    }
    
    // í† í° ê¸¸ì´ ê¸°ë°˜ ê°€ì¤‘ì¹˜
    const lengthWeight = Math.min(token.length / 10, 1.0);
    
    // ì–¸ì–´ë³„ íŠ¹ìˆ˜ ê°€ì¤‘ì¹˜
    const languageWeight = language === 'ko' ? 
      (token.match(/[ê°€-í£]/g) || []).length / token.length :
      (token.match(/[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]/g) || []).length / token.length;
    
    return 0.5 + lengthWeight * 0.3 + languageWeight * 0.2;
  }
  
  private normalizeVector(vector: number[]): number[] {
    // L2 ì •ê·œí™”
    const magnitude = Math.sqrt(vector.reduce((sum, val) => sum + val * val, 0));
    
    if (magnitude === 0) {
      return vector.map(() => 0);
    }
    
    return vector.map(val => val / magnitude);
  }
}

// ë©”ëª¨ë¦¬ ê¸°ë°˜ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
export class VectorDatabase {
  private embeddings: Map<string, VectorEmbedding> = new Map();
  private generator: EmbeddingGenerator = new EmbeddingGenerator();
  private indexByLanguage: Map<string, VectorEmbedding[]> = new Map();
  
  // ë¬¸ì„œ ì¶”ê°€ ë° ì¸ë±ì‹±
  async addDocument(document: DocumentChunk): Promise<void> {
    console.log(`ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¬¸ì„œ ì¶”ê°€ ì¤‘: ${document.metadata.source}`);
    
    try {
      // ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í•  (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì²­í¬ ì „ëµ ì‚¬ìš©)
      const chunks = this.chunkDocument(document);
      
      for (let i = 0; i < chunks.length; i++) {
        const chunk = chunks[i];
        const embeddingId = `${document.id}_chunk_${i}`;
        
        // ë²¡í„° ì„ë² ë”© ìƒì„±
        const vector = await this.generator.generateEmbedding(
          chunk, 
          document.metadata.language
        );
        
        const embedding: VectorEmbedding = {
          id: embeddingId,
          vector: vector,
          metadata: {
            documentId: document.id,
            chunkIndex: i,
            text: chunk,
            language: document.metadata.language,
            source: document.metadata.source,
            section: document.metadata.section,
            timestamp: new Date()
          }
        };
        
        this.embeddings.set(embeddingId, embedding);
        
        // ì–¸ì–´ë³„ ì¸ë±ìŠ¤ êµ¬ì¶•
        const langKey = document.metadata.language;
        if (!this.indexByLanguage.has(langKey)) {
          this.indexByLanguage.set(langKey, []);
        }
        this.indexByLanguage.get(langKey)!.push(embedding);
      }
      
      console.log(`ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ: ${chunks.length}ê°œ ì²­í¬ ìƒì„±`);
    } catch (error) {
      console.error(`ë¬¸ì„œ ì¸ë±ì‹± ì˜¤ë¥˜: ${document.metadata.source}`, error);
    }
  }
  
  // ë¬¸ì„œ ì²­í‚¹ (ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• )
  private chunkDocument(document: DocumentChunk): string[] {
    const text = document.content;
    const maxChunkSize = 300; // ìµœëŒ€ ì²­í¬ í¬ê¸°
    const overlapSize = 50;   // ì²­í¬ ê°„ ê²¹ì¹¨
    
    // ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
    const sentences = text.split(/[.ã€‚!?ï¼ï¼Ÿ\n]+/).filter(s => s.trim().length > 10);
    
    const chunks: string[] = [];
    let currentChunk = '';
    let sentenceBuffer: string[] = [];
    
    for (const sentence of sentences) {
      const trimmedSentence = sentence.trim();
      if (!trimmedSentence) continue;
      
      // í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€ ì‹œ í¬ê¸° í™•ì¸
      const potentialChunk = currentChunk + (currentChunk ? '. ' : '') + trimmedSentence;
      
      if (potentialChunk.length <= maxChunkSize) {
        currentChunk = potentialChunk;
        sentenceBuffer.push(trimmedSentence);
      } else {
        // í˜„ì¬ ì²­í¬ ì™„ì„±
        if (currentChunk) {
          chunks.push(currentChunk);
        }
        
        // ìƒˆ ì²­í¬ ì‹œì‘ (ê²¹ì¹¨ ì ìš©)
        const overlapSentences = sentenceBuffer.slice(-Math.ceil(overlapSize / 50));
        currentChunk = overlapSentences.concat([trimmedSentence]).join('. ');
        sentenceBuffer = overlapSentences.concat([trimmedSentence]);
      }
    }
    
    // ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
    if (currentChunk) {
      chunks.push(currentChunk);
    }
    
    return chunks.length > 0 ? chunks : [text]; // ìµœì†Œ 1ê°œ ì²­í¬ ë³´ì¥
  }
  
  // ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
  async search(query: string, language: 'ko' | 'ja', limit: number = 10): Promise<VectorSearchResult[]> {
    console.log(`ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰: "${query}" (${language})`);
    
    try {
      // ì¿¼ë¦¬ ë²¡í„° ìƒì„±
      const queryVector = await this.generator.generateEmbedding(query, language);
      
      // ì–¸ì–´ë³„ í›„ë³´ ì„ë² ë”© ì„ íƒ
      const candidates = this.getCandidateEmbeddings(language);
      console.log(`ğŸ“Š ê²€ìƒ‰ í›„ë³´: ${candidates.length}ê°œ ì„ë² ë”©`);
      
      // ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
      const results: VectorSearchResult[] = [];
      
      for (const embedding of candidates) {
        const similarity = this.calculateCosineSimilarity(queryVector, embedding.vector);
        const distance = 1 - similarity; // ê±°ë¦¬ëŠ” 1 - ìœ ì‚¬ë„
        
        console.log(`ğŸ“„ ë¬¸ì„œ "${embedding.metadata.text.substring(0, 50)}..." ìœ ì‚¬ë„: ${similarity.toFixed(3)}`);
        
        // ì„ê³„ê°’ í•„í„°ë§ (ë” ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë³€ê²½)
        if (similarity > 0.1) { // ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ ë‚®ì¶¤
          results.push({
            embedding,
            similarity,
            distance
          });
        }
      }
      
      // ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
      results.sort((a, b) => b.similarity - a.similarity);
      
      console.log(`ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: ${results.length}ê°œ ê²°ê³¼ ì¤‘ ìƒìœ„ ${limit}ê°œ ë°˜í™˜`);
      return results.slice(0, limit);
      
    } catch (error) {
      console.error('ë²¡í„° ê²€ìƒ‰ ì˜¤ë¥˜:', error);
      return [];
    }
  }
  
  // í›„ë³´ ì„ë² ë”© ì„ íƒ (ì–¸ì–´ ê¸°ë°˜ + í•˜ì´ë¸Œë¦¬ë“œ)
  private getCandidateEmbeddings(language: 'ko' | 'ja'): VectorEmbedding[] {
    const primaryCandidates = this.indexByLanguage.get(language) || [];
    const secondaryCandidates = this.indexByLanguage.get(language === 'ko' ? 'ja' : 'ko') || [];
    
    // ì£¼ ì–¸ì–´ 70% + ë³´ì¡° ì–¸ì–´ 30% ë¹„ìœ¨ë¡œ ê²€ìƒ‰
    const primaryCount = Math.ceil(primaryCandidates.length * 0.7);
    const secondaryCount = Math.floor(secondaryCandidates.length * 0.3);
    
    return [
      ...primaryCandidates.slice(0, primaryCount),
      ...secondaryCandidates.slice(0, secondaryCount)
    ];
  }
  
  // ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
  private calculateCosineSimilarity(vector1: number[], vector2: number[]): number {
    if (vector1.length !== vector2.length) {
      throw new Error('ë²¡í„° ì°¨ì›ì´ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤');
    }
    
    let dotProduct = 0;
    let magnitude1 = 0;
    let magnitude2 = 0;
    
    for (let i = 0; i < vector1.length; i++) {
      dotProduct += vector1[i] * vector2[i];
      magnitude1 += vector1[i] * vector1[i];
      magnitude2 += vector2[i] * vector2[i];
    }
    
    magnitude1 = Math.sqrt(magnitude1);
    magnitude2 = Math.sqrt(magnitude2);
    
    if (magnitude1 === 0 || magnitude2 === 0) {
      return 0;
    }
    
    return dotProduct / (magnitude1 * magnitude2);
  }
  
  // ë°ì´í„°ë² ì´ìŠ¤ í†µê³„ ì •ë³´
  getStats(): { totalEmbeddings: number; byLanguage: Record<string, number> } {
    const stats = {
      totalEmbeddings: this.embeddings.size,
      byLanguage: {} as Record<string, number>
    };
    
    this.indexByLanguage.forEach((embeddings, language) => {
      stats.byLanguage[language] = embeddings.length;
    });
    
    return stats;
  }
  
  // ë””ë²„ê·¸: ëª¨ë“  ì €ì¥ëœ ì„ë² ë”© ì •ë³´ ì¶œë ¥
  debugPrintAllEmbeddings(): void {
    console.log('=== ë²¡í„° DB ë””ë²„ê·¸ ì •ë³´ ===');
    console.log(`ì´ ì„ë² ë”© ìˆ˜: ${this.embeddings.size}`);
    
    let count = 0;
    this.embeddings.forEach((embedding, id) => {
      count++;
      console.log(`\n[ì„ë² ë”© ${count}] ID: ${id}`);
      console.log(`- ë¬¸ì„œ ID: ${embedding.metadata.documentId}`);
      console.log(`- ì²­í¬ ì¸ë±ìŠ¤: ${embedding.metadata.chunkIndex}`);
      console.log(`- ì–¸ì–´: ${embedding.metadata.language}`);
      console.log(`- ì†ŒìŠ¤: ${embedding.metadata.source}`);
      console.log(`- ì„¹ì…˜: ${embedding.metadata.section}`);
      console.log(`- í…ìŠ¤íŠ¸ (ì²˜ìŒ 200ì): ${embedding.metadata.text.substring(0, 200)}...`);
      console.log(`- ë²¡í„° ì°¨ì›: ${embedding.vector.length}`);
      console.log(`- ë²¡í„° ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ): [${embedding.vector.slice(0, 5).map(v => v.toFixed(3)).join(', ')}...]`);
    });
    
    console.log('\n=== ì–¸ì–´ë³„ ì¸ë±ìŠ¤ ===');
    this.indexByLanguage.forEach((embeddings, language) => {
      console.log(`${language}: ${embeddings.length}ê°œ ì„ë² ë”©`);
    });
  }
  
  // ë””ë²„ê·¸: íŠ¹ì • ì¿¼ë¦¬ì— ëŒ€í•œ ìƒì„¸ ê²€ìƒ‰ ì •ë³´
  async debugSearch(query: string, language: 'ko' | 'ja'): Promise<void> {
    console.log(`\n=== ë²¡í„° ê²€ìƒ‰ ë””ë²„ê·¸: "${query}" (${language}) ===`);
    
    // ì¿¼ë¦¬ ë²¡í„° ìƒì„±
    const queryVector = await this.generator.generateEmbedding(query, language);
    console.log(`ì¿¼ë¦¬ ë²¡í„° ìƒì„±ë¨ (ì°¨ì›: ${queryVector.length})`);
    console.log(`ì¿¼ë¦¬ ë²¡í„° ìƒ˜í”Œ: [${queryVector.slice(0, 5).map(v => v.toFixed(3)).join(', ')}...]`);
    
    // ëª¨ë“  ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
    const allResults: { embedding: VectorEmbedding; similarity: number }[] = [];
    
    this.embeddings.forEach((embedding) => {
      const similarity = this.calculateCosineSimilarity(queryVector, embedding.vector);
      allResults.push({ embedding, similarity });
    });
    
    // ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    allResults.sort((a, b) => b.similarity - a.similarity);
    
    console.log(`\nì „ì²´ ${allResults.length}ê°œ ì„ë² ë”©ê³¼ì˜ ìœ ì‚¬ë„ ê²°ê³¼:`);
    allResults.slice(0, 10).forEach((result, index) => {
      console.log(`\n[Top ${index + 1}] ìœ ì‚¬ë„: ${result.similarity.toFixed(4)}`);
      console.log(`- ì–¸ì–´: ${result.embedding.metadata.language}`);
      console.log(`- ì†ŒìŠ¤: ${result.embedding.metadata.source}`);
      console.log(`- í…ìŠ¤íŠ¸: ${result.embedding.metadata.text.substring(0, 150)}...`);
    });
  }
  
  // ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
  clear(): void {
    this.embeddings.clear();
    this.indexByLanguage.clear();
    console.log('ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤');
  }
}

// ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
export const vectorDatabase = new VectorDatabase();